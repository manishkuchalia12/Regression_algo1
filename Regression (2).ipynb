{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29b1d17-60f4-4e07-96bf-8e37d3cec231",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "Ans:-Ridge Regression, also known as Tikhonov regularization or L2 regularization, is a linear regression technique that introduces a regularization term to the ordinary least squares (OLS) regression cost function. The goal of Ridge Regression is to prevent overfitting by adding a penalty term that discourages the coefficients of the regression model from becoming too large.\n",
    "\r\n",
    "Key Features of Ridge Regression:\r\n",
    "Regularization Teifferences from Ordinary Least Squares (OLS) Regression:\r\n",
    "Regularization Term:\r\n",
    "\r\n",
    "Ridge Regression introduces a regularization term to the cost function, wereas OLS regression does not include any regularization.\r\n",
    "Coefficient Shrinkage:\r\n",
    "\r\n",
    "Ridge Regression results in shrinkage of the coefficients towards zero, reducing their magnitudes, while OLS egression estimates coefficients without any shrinkage.\r\n",
    "Handling Multicollinearity:\r\n",
    "\r\n",
    "Ridge Regression is effective in handling multicollinearity (high correlation among predictor variables) by preventing the coefficients from becoming too large. OLS can be sensitive to multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab907a56-30ba-43d7-9f2d-c3b98a8e49da",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?\n",
    "Ans:-Ridge Regression, like ordinary least squares (OLS) regression, relies on several assumptions to ensure the validity and effectiveness of the model. The key assumptions of Ridge Regression are similar to those of linear regression:\n",
    "\r\n",
    "Linearit:\r\n",
    "\r\n",
    "The relationship between the independent variables and the dependent variable is assumed to be linear. Ridge Regression is an extension of linear regression, and it requires a linear relationship to provide meaningful results.\r\n",
    "Independence of Erors:\r\n",
    "\r\n",
    "The errors (residuals) should be independent of each other. In other words, the error in predicting one observation should not provide information about the error in predicting another observation.\r\n",
    "Homoscedaticity:\r\n",
    "\r\n",
    "The variance of the errors should be constant across all levels of the independent variables. This assumption ensures that the spread of residuals is consistent throughout the range of predicted values.\r\n",
    "Normalityof Errors:\r\n",
    "\r\n",
    "While Ridge Regression does not assume normality of the predictors or the response variable, it assumes that the errors are normally distributed. This assumption is important for making statistical inferences and constructing confidence intervals.\r\n",
    "No Perfect Multcollinearity:\r\n",
    "\r\n",
    "Perfect multicollinearity occurs when two or more independent variables are perfectly correlated, leading to mathematical issues in estimating coefficients. While Ridge Regression can handle multicollinearity, it is assumed that there is no perfect multicollinearity.\n",
    "No Endogeneity:\r\n",
    "\r\n",
    "The independent variables are assumed to be exogenous, meaning they are not influenced by the errors. This assumption is crucial for causal inference.\r\n",
    "Fixed Nmber of Predictors:\r\n",
    "\r\n",
    "Ridge Regression assumes that the number of predictors is fixed and does not increase with the sample size. This assumption is relevant for the interpretation of regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9dde57-d6cd-4f8d-b7fc-77d9c13ada88",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094e14f-658c-4795-a236-fa777bd8f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up Ridge Regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define a range of alpha (equivalent to lambda in Ridge Regression)\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(ridge, param_grid={'alpha': alphas}, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha\n",
    "best_alpha = grid_search.best_params_['alpha']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa046bfc-965a-47d7-bcc3-e46909e929aa",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "Ans:-Yes, Ridge Regression can be used for feature selection, but it differs from methods like Lasso Regression in the way it handles feature selection. While Lasso tends to drive some coefficients exactly to zero, resulting in a sparse model, Ridge Regression does not exactly zero out coefficients. Instead, it shrinks them towards zero, leading to small but non-zero coefficients.\r\n",
    "\r\n",
    "However, Ridge Regression still provides a form of feature selection by effectively downweighting less important features. Features that contribute less to the model's predictive performance are assigned smaller coefficients in Ridge Regression. Here's how Ridge Regression influences feature selection:\r\n",
    "\r\n",
    "Shrinkage of Coefficients:\r\n",
    "\r\n",
    "Ridge Regression includes a regularization term that penalizes large coefficients. As the regularization par(r (\r\n",
    "�\r\n",
    "λ) increases, the shrinkage effect becomes more pronounced.\r\n",
    "Larger coefficients are penalized more heavily, and the model tends to assign smaller magnitudes to less influential features.\r\n",
    "Trade-off between Fit and Coplexity:\r\n",
    "\r\n",
    "The regularization term in Ridge Regression introduces a trade-off between fitting the training data well and keeping the modeimple.\r\n",
    "As \r\n",
    "�\r\n",
    "λ increases, the model becomes more regularized, and the coefficients tend to be pushed towards zero. This leads to a smoother model with a reduced risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea0bf7-062f-4620-bf46-0e1111c2e7a9",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "Ans:-\r\n",
    "Ridge Regression is particularly useful in the presence of multicollinearity, which refers to high correlation among the independent variables in a regression model. Multicollinearity can lead to instability in the estimation of coefficients, making it challenging to interpret the individual contributions of each predictor variable. Ridge Regression addresses this issue by introducing a regularization term that penalizes large coefficients\r\n",
    "\r\n",
    "Performance of Ridge Regression in the Presence of Multicollineari\n",
    "Shrinkage of Coefficients:\n",
    "\r\n",
    "The regularization term in Ridge Regression induces a shrinkage effect on the coefficients. While it does not drive coefficients exactly to zero, it reduces their magnitudes.\r\n",
    "This shrinkage is advantageous when dealing with multicollinearity, as it dampens the impact of highly correlated features.\r\n",
    "Balancing Fit and Complexty:\r\n",
    "\r\n",
    "Ridge Regression provides a trade-off between fitting the training data well and keeping the model simple. The regularization paramr (\r\n",
    "�\r\n",
    "λ) controls the strength of the penalty.ty:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f1f34-8cdf-498e-838f-223b4cc8bb1e",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cf0a2-bf8a-45a5-8839-90575f67fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Generate a synthetic dataset with both continuous and categorical variables\n",
    "np.random.seed(42)\n",
    "data_size = 100\n",
    "X_continuous = np.random.rand(data_size, 2)  # Two continuous features\n",
    "X_categorical = np.random.choice(['A', 'B', 'C'], size=(data_size, 1))  # One categorical feature\n",
    "X = np.concatenate([X_continuous, X_categorical], axis=1)\n",
    "y = 2 * X_continuous[:, 0] + 3 * X_continuous[:, 1] + np.random.normal(0, 0.1, size=data_size)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing for continuous and categorical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), [0, 1]),  # Scaling continuous variables\n",
    "        ('cat', OneHotEncoder(), [2])       # One-hot encoding categorical variable\n",
    "    ])\n",
    "\n",
    "# Create a Ridge Regression model in a pipeline with preprocessing\n",
    "ridge_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0))  # You can adjust the alpha parameter based on your needs\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R^2 (Testing):\", r2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
